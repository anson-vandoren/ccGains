#!/usr/bin/env python
# -*- coding:utf-8 -*-
#
# ----------------------------------------------------------------------
# ccGains - Create capital gains reports for cryptocurrency trading.
# Copyright (C) 2017 JÃ¼rgen Probst
#
# This file is part of ccGains.
#
# ccGains is free software: you can redistribute it and/or modify it
# under the terms of the GNU Lesser General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# ccGains is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with ccGains. If not, see <http://www.gnu.org/licenses/>.
# ----------------------------------------------------------------------
#
# Get the latest version at: https://github.com/probstj/ccGains
#

from os import path
import pandas as pd
import requests
from time import sleep
from dateutil import tz

import logging

log = logging.getLogger(__name__)


def resample_weighted_average(
        df, freq, data_col, weight_col, include_weights=False):
    """Resample a DataFrame with a DatetimeIndex. Return weighted
    averages of groups.

    :param df:
        The pandas.DataFrame to be resampled
    :param freq:
        The new frequency of the resampled time series
    :param data_col:
        Column to take the average of
    :param weight_col:
        Column with the weights
    :param include_weights: (default False)
        If True, include the summed weights in result
    :return:
        if include_weights:

          pandas.DataFrame with two columns:

            - data_col: the weighted averages,
            - weight_col: the summed weights

        else:

          pandas.Series with weighted averages

    Source: ErnestScribbler, https://stackoverflow.com/a/44683506

    """
    # Create a new column with data * weight. The trick is that
    # this column will be grouped along with the other data and
    # can thus be used in aggregation:
    df['data_times_weight'] = df[data_col] * df[weight_col]
    g = df.resample(freq)
    avgs = g['data_times_weight'].sum() / g[weight_col].sum()
    del df['data_times_weight']
    if include_weights:
        return pd.DataFrame(
            {data_col: avgs, weight_col: g[weight_col].sum()})
    else:
        return avgs


class HistoricData(object):
    def __init__(self, unit):
        """Create a HistoricData object with no data.
        The unit must be a string given in the form
        'currency_one/currency_two', e.g. 'EUR/BTC'.

        Only use this constructor if you want to manually set
        the data, otherwise use one of the subclasses
        `HistoricDataCSV` or `HistoricDataAPI`.

        To manually set data, self.data must be a pandas time series
        with a fixed frequency.

        """
        try:
            self.cto, self.cfrom = unit.upper().split('/')
        except:
            raise ValueError(
                'Please supply the currency exchange rate unit '
                'in the correct form, e.g. "EUR/USD"')
        self.unit = self.cto + '/' + self.cfrom
        self.interval = None
        self.data = None

    def prepare_request(self, dtime):
        """Return a Pandas DataFrame which contains the data at the
        requested datetime *dtime*.

        """
        return self.data

    def get_price(self, dtime):
        """Return the price at datetime *dtime*"""
        df = self.prepare_request(dtime)
        return df.at[pd.Timestamp(dtime).floor(df.index.freq)]


class HistoricDataCSV(HistoricData):

    def __init__(self, file_name, unit, interval='H'):
        """Initialize a HistoricData object with data loaded from a csv
        file. The unit must be a string given in the form
        'currency_one/currency_two', e.g. 'EUR/BTC'.
        The csv must consist of three columns: first a unix timestamp,
        second the rate given in *unit*, third the amount traded.
        (Such a csv can be downloaded from bitcoincharts.com)

        The file may also be compressed and will be deflated on-the-fly;
        allowed extensions are: '.gz', '.bz2', '.zip' or '.xz'.

        The data will be resampled by calculating the weighted price
        for interval steps specified by *interval*. See:
        http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases
        for possible values.

        For faster loading times, a HDF5 file is created from the csv
        the first time it is loaded and used transparently the next
        time an HistoricData object is created with the same csv. If
        the csv file is newer than the HDF5 file, the latter will be
        updated.

        """
        super(HistoricDataCSV, self).__init__(unit)
        self.interval = interval
        self.dataset = '{0:s}_{1:s}'.format(self.cto, self.cfrom)

        fbase, fext = path.splitext(file_name)
        if fext != '.h5':
            # For faster loading, convert 'csv' file to HDF5 and load the
            # latter, unless the 'csv' file is newer:
            try:
                csvtime = path.getmtime(file_name)
            except OSError:
                csvtime = 0
            try:
                h5time = path.getmtime(fbase + '.h5')
            except OSError:
                h5time = 0
            if csvtime == 0 and h5time == 0:
                raise IOError('File does not exist: %s' % file_name)

            if csvtime <= h5time:
                # Quick load from h5 file, but only if data matches:
                self.file_name = fbase + '.h5'
                try:
                    with pd.HDFStore(self.file_name, mode='r') as store:
                        self.data = store[self.dataset]
                except (KeyError, AttributeError, IOError):
                    # Will force csv to be reloaded:
                    h5time = 0

            if csvtime > h5time:
                self.data = pd.read_csv(
                    file_name,
                    header=None, index_col='time',
                    names=['time', self.unit, 'volume'])
                # parse timestamps:
                # (quicker than doing it directly in pd.read_csv)
                self.data.index = pd.to_datetime(
                    self.data.index, unit='s', utc=True)
                # sort the data by time:
                self.data.sort_index(inplace=True)
                # create new HDF5 file:
                self.file_name = fbase + '.h5'
                with pd.HDFStore(self.file_name) as store:
                    store[self.dataset] = self.data

        # Get weighted prices, resampled with interval:
        # (this will only return one column, the weighted prices; the
        # total volume won't be needed anymore)
        self.data = resample_weighted_average(
            self.data, interval, self.unit, 'volume')

        # In case the data has been upsampled (Some events beeing more
        # separated than interval), the resulting Series will have some
        # NaNs. Forward-fill them with the last prices before:
        self.data.ffill(inplace=True)

        # Don't change self.data's DateTimeIndex into PeriodIndex since
        # periods don't support timezones, which we want to keep.
        # (https://github.com/pandas-dev/pandas/issues/2106)


class HistoricDataAPI(HistoricData):
    def __init__(self, cache_folder, unit, interval='H'):
        """Initialize a HistoricData object which tranparently fetch data
        on request (`get_price`) from the public Poloniex API:
        https://poloniex.com/public?command=returnTradeHistory

        For faster loading times on future calls, a HDF5 file is created
        from the requested data and used transparently the next time a
        request for the same day and pair is made. These HDF5 files are
        saved in *cache_folder*.

        The *unit* must be a string given in the form
        'currency_one/currency_two', e.g. 'EUR/BTC'.

        The data will be resampled by calculating the weighted price
        for interval steps specified by *interval*. See:
        http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases
        for possible values.



        """
        super(HistoricDataAPI, self).__init__(unit)
        self.interval = interval
        self.url = 'https://poloniex.com/public'
        # Poloniex does not allow more than 6 queries per second;
        # Wait at least this number of seconds between queries:
        self.query_wait_time = 0.17
        # Poloniex limits the amount of trades returned per query:
        self.max_trades_per_query = 50000
        self.command = 'returnTradeHistory'
        self.currency_pair = '{0.cto:s}_{0.cfrom:s}'.format(self)
        self.last_query_time = pd.Timestamp.now()
        self.connection_error = requests.ConnectionError(
            'Price data for %s could not be loaded from %s '
            '- are you online?' % (self.currency_pair, self.url))
        file_name = path.join(
            cache_folder,
            'Poloniex_{0.cto:s}_{0.cfrom:s}_{0.interval:s}.h5'.format(self))
        # flipped currency pair:
        file_name_f = path.join(
            cache_folder,
            'Poloniex_{0.cfrom:s}_{0.cto:s}_{0.interval:s}.h5'.format(self))
        # See if the currency pair is already cached:
        if path.exists(file_name):
            self.file_name = file_name
        elif path.exists(file_name_f):
            # flip currency pair:
            self.cfrom, self.cto = self.cto, self.cfrom
            self.unit = self.cto + '/' + self.cfrom
            self.file_name = file_name_f
            self.currency_pair = '{0.cto:s}_{0.cfrom:s}'.format(self)
        else:
            # Query the api to see if the pair is available:
            try:
                req = requests.get(
                    self.url,
                    params={'command': 'returnTicker'})
            except requests.ConnectionError:
                raise self.connection_error
            if self.currency_pair in req.json():
                self.file_name = file_name
            else:
                # try if flipped currency pair is available:
                currency_pair_f = '{0.cfrom:s}_{0.cto:s}'.format(self)
                if not currency_pair_f in req.json():
                    raise ValueError(
                        'Neither currency pair "{0:s}" nor pair "{1:s}" is '
                        'available on "{2:s}".'.format(
                            self.currency_pair, currency_pair_f, self.url))
                # flip currency pair:
                self.cfrom, self.cto = self.cto, self.cfrom
                self.unit = self.cto + '/' + self.cfrom
                self.currency_pair = currency_pair_f
                self.file_name = file_name_f

    def _fetch_from_api(self, start, end=None):
        """Fetch historical trading data from API.

        :param start: UNIX Timestamp (seconds); Start of range to fetch.
        :param end: UNIX timestamp (seconds); End of range to fetch.
            If None (default), will fetch a range of one day, i.e.
            end will be `start + 86400`.
        :returns: tuple (number of fetched trades, resampled data)

        The returned data will be resampled to `self.interval`. The
        data will consist of weighted historical prices, averaged over
        each interval using the traded amounts as weights.

        In case there is a limit on the number of trades returned by
        the API, not the full requested range from `start` to `end`
        can be returned by this function. Check the returned
        data and the number of fetched trades to determine if data
        is missing. In such a case, the weighted averages of prices
        at the missing ends of the range cannot be trusted.

        """
        if end is None:
            # fetch a time span of one day:
            end = start + 86400
        # Wait for the min call time to pass:
        now = pd.Timestamp.now()
        delta = (now - self.last_query_time).total_seconds()
        self.last_query_time = now
        if delta <= self.query_wait_time:
            log.info('waiting %f s', self.query_wait_time - delta)
            sleep(self.query_wait_time - delta)
            log.info('continuing')
        # Make request:
        try:
            req = requests.get(
                self.url,
                params={'command': self.command,
                        'currencyPair': self.currency_pair,
                        'start': int(start),
                        'end': int(end)})
        except requests.ConnectionError:
            raise self.connection_error
        log.info('Fetched historical price data with request: %s', req.url)
        try:
            df = pd.read_json(
                req.text, orient='records', precise_float=True,
                convert_axes=False, convert_dates=['date'],
                keep_default_dates=False, dtype={
                    'tradeID': False, 'globalTradeID': False,
                    'total': False, 'type': False, 'date': False,
                    'rate': float, 'amount': float})
            log.info('Successfully fetched %i trades', len(df))
        except ValueError:
            # There might have been an error returned from Poloniex,
            # which cannot be parsed the same way than regular data.
            j = pd.io.json.loads(req.text)
            if 'error' in j:
                raise ValueError(
                    "Poloniex API returned error: {0:s}\n"
                    "Requested URL was: {1:s}".format(
                        j['error'],
                        req.url))
            else:
                raise
        df.set_index('date', inplace=True)

        # Get weighted prices, resampled with interval:
        # (this will only return one column, the weighted prices;
        # the total volume won't be needed anymore)
        data = resample_weighted_average(
            df, self.interval, 'rate', 'amount')

        # In case the data has been upsampled (Some events
        # beeing more separated than interval), the resulting
        # Series will have some NaNs. Forward-fill them with
        # the last prices before:
        data.ffill(inplace=True)

        if data.index.tzinfo is None:
            data.index = data.index.tz_localize('UTC')

        return len(df), data

    def prepare_request(self, dtime):
        """Return a Pandas DataFrame which contains the data for the
        requested datetime *dtime*.

        """
        dtime = pd.Timestamp(dtime).tz_convert(tz.tzutc())
        key = "d{a:04d}{m:02d}{d:02d}".format(
            a=dtime.year, m=dtime.month, d=dtime.day)
        with pd.HDFStore(self.file_name, mode='a') as store:
            if key in store:
                try:
                    self.data = store.get(key)
                    # Check whether the data can be accessed:
                    self.data.at[
                        pd.Timestamp(dtime).floor(self.data.index.freq)]
                    return self.data
                except (KeyError, AttributeError):
                    # In case the hdf5 file got corrupted somehow,
                    # with the requested date missing from the data,
                    # reload the data from the API:
                    log.warning(
                        'Date %s missing in cached data. '
                        'Repeating request to API', dtime)

            # We need to fetch the data from the poloniex api:
            start = dtime.floor('D').value // 10 ** 9
            count, self.data = self._fetch_from_api(start)

            # Did we reach the limit?
            while count == self.max_trades_per_query:
                # The API might not have returned all requested trades.
                # If Poloniex omits data, the end of the requested range
                # is returned.
                # Remove first faulty interval:
                # (faulty because data might be missing)
                del self.data[self.data.index[0]]
                # end time of next request:
                end = self.data.index[0].value // 10 ** 9 - 1
                # new request:
                count, data = self._fetch_from_api(start, end)
                if len(data) <= 1 and count == self.max_trades_per_query:
                    # It seems our interval is too big or there are just
                    # too many trades in the interval, so that we cannot
                    # fetch one interval with a single request.
                    raise Exception(
                        "There are too many trades in the chosen "
                        "interval of %s ending on %s. Please try again "
                        "with an HistoricDataAPI object with smaller "
                        "interval size." % (
                            self.data.index.freq,
                            self.data.index[0]))
                self.data = self.data.combine_first(data)

            store.put(key, self.data, format="fixed")
            return self.data


class HistoricDataAPIBinance(HistoricData):
    def __init__(self, cache_folder, unit, interval='H'):
        """Initialize a HistoricData object which will transparently fetch
        data on request (`get_price`) from the public Binance API:
        https://api.binance.com/api/v1/aggTrades

        For faster loading times on future calls, a HDF5 file is created
        from the requested data and used transparently the next time a
        request for the same day and pair is made. These HDF5 files are
        saved in *cache_folder*.

        The *unit* must be a string in the form
        'currency_one/currency_two', e.g. 'NEO/BTC'.

        The data will be resampled by calculating the weighted price
        for interval steps specified by *interval*. See:
        http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases
        for possible values.


        """
        super(HistoricDataAPIBinance, self).__init__(unit)
        self.interval = interval
        self.url = 'https://api.binance.com/api/v1'
        # Binance imposes limits on (non-order) requests as:
        # 1200 weighted requests per 1 minute, or
        # 5000 raw requests per 5 minutes
        # Aggregate trade data is 1-weighted, so limit is 1000/min
        # Wait at least this many seconds between queries:
        self.query_wait_time = 0.06
        # Binance limits number of aggregate trades returned per query:
        self.max_trades_per_query = 1000
        self.command = 'aggTrades'
        # Binance does not use any separator between base and quote assets
        self.currency_pair = '{0.cto:s}{0.cfrom:s}'.format(self)
        self.last_query_time = pd.Timestamp.now()
        self.connection_error = requests.ConnectionError(
            'Price data for %s could not be loaded from %s '
            '- are you online?' % (self.currency_pair, self.url))
        file_name = path.join(
            cache_folder,
            'Binance_{0.cto:s}{0.cfrom:s}_{0.interval:s}.h5'.format(self))
        # Flipped currency pair:
        file_name_f = path.join(
            cache_folder,
            'Binance_{0.cfrom:s}{0.cto:s}_{0.interval:s}.h5'.format(self))
        # See if the currency pair is already cached:
        if path.exists(file_name):
            self.file_name = file_name
        elif path.exists(file_name_f):
            # Flip currency pair
            self.cfrom, self.cto = self.cto, self.cfrom
            self.unit = self.cto + '/' + self.cfrom
            self.file_name = file_name_f
            self.currency_pair = '{0.cto:s}{0.cfrom:s}'.format(self)
        else:
            # Query the API to see if the pair is available:
            try:
                req = requests.get(self.url + '/exchangeInfo')
            except requests.ConnectionError:
                raise self.connection_error
            known_symbols = [info['symbol'] for info in req.json()['symbols']]
            if self.currency_pair in known_symbols:
                self.file_name = file_name
            else:
                # Try flipped currency pair
                currency_pair_f = '{0.cfrom:s}{0.cto:s}'.format(self)
                if currency_pair_f not in known_symbols:
                    raise ValueError(
                        'Neither currency pair "{0:s}" nor pair "{1:s}" is '
                        'available on "{2:s}".'.format(
                            self.currency_pair, currency_pair_f, self.url))
                # Flip currency pair
                self.cfrom, self.cto = self.cto, self.cfrom
                self.unit = self.cto + '/' + self.cfrom
                self.currency_pair = currency_pair_f
                self.file_name = file_name_f

    def _wait_if_needed(self):
        # Wait for the minimum call time to pass:
        now = pd.Timestamp.now()
        delta = (now - self.last_query_time).total_seconds()
        self.last_query_time = now
        if delta <= self.query_wait_time:
            log.info('waiting %f s', self.query_wait_time - delta)
            sleep(self.query_wait_time - delta)
            log.info('continuing')

    def _fetch_from_api(self, start, end=None):
        """Fetch historical trading data from API.

        :param start: UNIX timestamp (seconds); Start of range to fetch.
        :param end: UNIT timestamp (seconds); End of range to fetch.
            If None (default), will fetch a range of one day, i.e.
            end will be `start + 86400`.
        :returns: tuple (number of fetched trades, resampled data)

        The returned data will be resampled to `self.interval`. The
        data will consist of weighted historical prices, averaged over
        each interval using the traded amounts as weights.

        In case there is a limit on the number of trades returned by
        the API, several queries will be sent to capture the full
        range of prices. Binance limits aggtrades to 1000 per response

        """
        if end is None:
            # Fetch a tme span of one day:
            end = start + 86400
        # Binance uses milliseconds instead of seconds
        start = int(start * 1000)
        end = int(end * 1000)
        self._wait_if_needed()

        # Get all the aggregate trades that happened in this time period for
        # this symbol

        # Make the first request. If we give a start time (we need to), then
        # we must also give an end time, and the difference must not be more
        # than one hour; this means we will likely get less than the limit
        # for the first request. Requests after the initial request can be
        # queried by id instead, and so we can get the maximum
        try:
            req = requests.get(
                self.url + '/aggTrades',
                params={'symbol': self.currency_pair,
                        'startTime': start,
                        'endTime': start + 3600 * 1000,
                        'limit': 1000})
        except requests.ConnectionError:
            raise self.connection_error
        print('Status: ', req.status_code)
        if req.status_code in [429, 418]:
            raise ConnectionError('Binance Rate limits exceeded')
        df = self._req_to_df(req)
        last_dt = df.index[-1]
        last_id = df.iloc[-1].loc['a']
        while last_dt < pd.Timestamp(end, unit='ms'):
            self._wait_if_needed()
            try:
                req = requests.get(
                    self.url + '/aggTrades',
                    params={'symbol': self.currency_pair,
                            'fromId': last_id + 1,
                            'limit': 1000})
            except requests.ConnectionError:
                raise self.connection_error
            print('Status: ', req.status_code)
            if req.status_code in [429, 418]:
                raise ConnectionError('Binance Rate limits exceeded')
            df = df.append(self._req_to_df(req))
            last_dt = df.index[-1]
            last_id = df.iloc[-1].loc['a']
        # All trades for the interval are now saved
        # May be trades past the requested date, so remove those
        df = df[df.index <= pd.Timestamp(end, unit='ms')]
        log.info('Fetched historical price data records with request: %s', req.url)

        # Get weighted prices, resampled with interval:
        # (this will only return one column, the weighted prices;
        # the total volume won't be needed anymore)
        data = resample_weighted_average(
            df, self.interval, 'p', 'q')

        # In case the data has been upsampled (some events
        # being more separated than interval), the resulting
        # Series will have some NaNs. Forward-fill them
        # with the last prices before:
        data.ffill(inplace=True)

        if data.index.tzinfo is None:
            data.index = data.index.tz_localize('UTC')

        return len(df), data

    @staticmethod
    def _req_to_df(req):
        # Read data into a pandas DataFrame
        try:
            df = pd.read_json(
                req.text, orient='records', precise_float=True,
                convert_axes=False, convert_dates=['T'], date_unit='ms',
                keep_default_dates=False, dtype={
                    'a': int,
                    'p': float,
                    'q': float,
                    'f': False,
                    'l': False,
                    'T': False,
                    'm': False,
                    'M': False
                     })
            log.info('Successfully fetched %i trades', len(df))
        except ValueError:
            # There might have been an error returned from Binance,
            # which cannot be parsed the same way as regular data.
            j = pd.io.json.loads(req.text)
            if 'error' in j:
                raise ValueError(
                    "Binance API returned error: {0:s}\n"
                    "Requested URL was: {1:s}".format(
                        j['error'], req.url))
            else:
                raise
        df.set_index('T', inplace=True)
        return df

    def prepare_request(self, dtime):
        """Return a pandas DataFrame which contains the data for the
        requested datetime *dtime*.

        """
        dtime = pd.Timestamp(dtime).tz_convert(tz.tzutc())
        key = "d{a:04d}{m:02d}{d:02d}".format(
            a=dtime.year, m=dtime.month, d=dtime.day)
        with pd.HDFStore(self.file_name, mode='a') as store:
            if key in store:
                try:
                    self.data = store.get(key)
                    # Check whether data can be accessed:
                    self.data.at[pd.Timestamp(dtime).floor(self.data.index.freq)]
                    return self.data
                except (KeyError, AttributeError):
                    # In case of the hdf5 file got corrupted somehow,
                    # with the requested date missing from the data,
                    # reload the data from the API:
                    log.warning(
                        'Date %s missing in cached data. '
                        'Repeating request to API', dtime)

            # We need to fetch the data from the Binance API
            start = dtime.floor('D').value // 10 ** 9
            count, self.data = self._fetch_from_api(start)

            # For Binance, _fetch_from_api() already ensures that all data is collected
            # (repeating subsequent queries if needed)

            store.put(key, self.data, format="fixed")
            return self.data
